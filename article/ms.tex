\documentclass[useAMS,usenatbib]{mn2e}

\usepackage{amsmath,amsbsy}
\usepackage{gitinfo2}

% Comment

% Convenient colloquiualisms.
\newcommand\article{\textit Article}
\newcommand\tc{\textit{The Cannon} }

% Convenient maths.
\newcommand\K{$\mathbf{K}$}
\newcommand\lv{\mathbf{\boldsymbol\ell_n}}
\newcommand\cv{{\boldsymbol\theta}_\lambda}
\newcommand\wv{\mathbf{w_n}}
\newcommand\given{|}

\newcommand\teff{$T_{\rm eff}$}
\newcommand\logg{$\log{g}$}
\newcommand\mh{${\mathrm [M/H]}$}

\title[Cannon Chemistry]{Detailed Chemical Abundances with \tc} 
% Some arrangement of the following authors:
\author[Casey et al.]{A.~R.~Casey$^1$, M.~Ness$^2$, G.~Gilmore$^1$,
    D.~W.~Hogg$^{2,3,4}$, H.~W.~Rix$^2$ \\ 
$^1$Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge
    CB3 0HA, UK\\
$^2$Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg,
    Germany\\
$^3$Center for Cosmology and Particle Physics, Department of Physics, New York
    University, 4 Washington Pl., room 424, New York, \\
    NY, 10003, USA\\
$^4$Center for Data Science, New York University, 726 Broadway, 7th Floor,
    New York, NY 10003, USA}
\begin{document}

\date{Accepted 2015 XX XX. Received 2015 October XX; in original form 2015 XX xx}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2015}

\maketitle

\label{firstpage}

\begin{abstract}
More than 15 million stellar spectra are expected to be acquired by ground-based surveys in
the coming decade. The detailed chemical abundance analysis of these data poses
a significant challenge. \tc provides a purely data-driven
approach to stellar parameter (label) determination. Cross-validation tests
demonstrate that the precision achievable is comparable to existing methods that
rely on model atmospheres and spectral synthesis, but \tc is $\sim$300,000 times
faster, making it perfectly applicable for surveys. Including individual abundances by extending \tc dimensionality can create enormous model difficulties, severely limiting the interpretability of the results. Here
we describe an alternative approach to including individual element abundances
in \tc. Using modest simplifications of the physics of spectral line formation,
we demonstrate precisely \textit{why} an approach like \tc works so effectively,
and provide a physically-motivated model that describes flux values, given the
stellar parameters. The inclusion of individual chemical abundances does not
impact on model flexibility, allowing for simultaneous training on detailed
chemical abundances (which are formed through well-understood interactions), as well as arbitrary labels where the impact on the spectra
is unknown or poorly understood (e.g., masses, ages). Crucially, we provide a direct link
between data-driven models and theory of model atmospheres and spectral line formation. We demonstrate that
detailed chemical abundances can be measured with a precision of 0.0X-0.0Y~dex
in Z APOGEE spectra on a modern CPU in just one minute. We present abundance
measurements of 15 elements in Z APOGEE spectra from DR12.
\end{abstract}

\begin{keywords}
\end{keywords}

\section{Introduction}
\label{sec:introduction}

Ground-based stellar spectroscopic surveys have enormous scientific potential. Key absorption features in stellar spectra permit inferences on the internal structure of stars and their detailed chemical composition. Given a large enough sample of stars, these inferences can directly guide or constrain models over a large range of physical scales: the formation of planets, stars and clusters, supernovae yields and nucleosynthetic pathways, as well as the large scale evolution of the Milky Way. For these reasons an increasing number of stellar spectroscopic surveys are being undertaken, with each survey generation increasing in sample size \citep{gaia_eso; galah; lamost; apogee; 4most}.

% Data have outgrown the methods.
In some ways, the spectroscopic data have outgrown the methods used to analyse them. High-resolution (R > 20,000) stellar spectroscopy is colloquially considered to be a `dark art': analyses are carefully performed by an expert spectroscopist, who themselves were trained by an expert in exactly where the stellar continuum should be placed, and which atomic lines are considered 'clean' or 'dirty'. This work is equally impressive as it is subjective: the best spectroscopists host an encyclopaedic knowledge of atomic absorption lines with decades of experience, yet in blind-tests their decisions of where to place the continuum quickly becomes the most dominant source of systematic error.
% Lot of unextracted information in these spectra (e.g., mass, ages)

% Chemical abundance analysis takes time.
Classical spectroscopic analyses are time-intensive both in human and computing resources. The Gaia-ESO Survey has been particularly instrumental in minimising the human resources required for analysis: all analysis codes of FGK stars employed in Gaia-ESO are now completely automatised. The computing challenge remains: some analysis codes take up to one hour (single core) to determine stellar parameters (\teff, \logg, \mh) for one star. Given the data scale (e.g., $\sim$400 stars observed simultaneously with HERMES), this poses a modest computational problem.

% These analysis techniques are not optimal, either.
These analyses also have strict limitations in validity, and a range of options are available. The community standard is to perform chemical abundance analyses using 1D model photospheres under the assumption of local thermal equilibrium (LTE). There are a number of photospheric models available. Detailed information about the species (atomic and molecular) opacities is required, and partition functions are usually approximated from polynomials. Atomic data (e.g., oscillator strengths) must be well-known, and the treatment of Stark and XX broadening must be carefully included. Results are further sensitive to the manner in which the equation of state was solved. Simply put, there are many things to consider for a careful analysis.

% TC is in stark contrast to these methods
% It's fast, accurate and precise.

% It ignores physics.
\tc{} falls into the regime of supervised machine learning. The results obtained by \tc{} thus far are extremely impressive, particularly when it is considered that the model ignores everything learned about atomic physics and stellar spectra since the names' inconception 100 years ago. There is no knowledge of stars, no input on interactions between fields and particles, nothing considering spectral line formation, and no input of atomic data, opacities, partition functions or depth-dependent photospheric quantities. Only a polynomial model of stellar parameter labels is required in order to reasonably approximate expensive spectral syntheses. However because \tc is purely data-driven, there is a missing link to physics.

% Including chemical abundances into TC is non-trivial.


% stellar parameter and chemical abundance analysis take time
% equation of state, radiative transfer & spectral synthesis of multiple lines
% 1D model atmospheres, 3D models.
% introducing tc
% it's awesome: fast, accurate and precise (given the models published thus far)
% impressively it does all this whilst ignoring all advances made in stellar physics over the past 100 years.
% introducing chemical abundances into TC is not trivial
% high dimensionality

\section{Model}

\tc{} provides a purely data-driven approach to stellar label determination. The
spectral model describes the flux $f_{n\lambda}$ for the $n$-th star a pixel with rest-wavelength $\lambda$ to be a function
of a coefficient vector $\cv$, and a label vector $\lv$:

\begin{equation}
    f_{n\lambda} = g(\lv\given\cv) + {\rm noise}.
\end{equation}

This form is flexible: the label vector $\lv$ may be linear in just a few common
labels (e.g., effective temperature $T_{\rm eff}$, surface gravity $\log{g}$, and
metallicity [M/H]\footnote{\citet{Ness2015} use [Fe/H] in $\lv$, but for the 
sake of distinguishing between overall metallicity and individual abundances,
we opt for [M/H] to describe overall metallicity throughout this \article{}})
or a complex combination of all $K$ available labels, some of which may have an
 unknown or non-trivial relationship with spectral fluxes \citep[e.g., mass, 
 ages, see][]{ness_2015b}. The underlying principle
is that the flux for any pixel from any star is predictable with a simple (usually
 a polynomial) model, given the labels. Indeed, \citet{Ness2015}
employ a quadratic-in-labels spectral model using stellar
parameters ($T_{\rm eff}$, $\log{g}$, [M/H])
and demonstrate through cross-validation that these labels can be 
measured with a precision of 95~K in $T_{\rm eff}$, 0.24 in $\log{g}$, 0.08 in 
[M/H], with negligible biases. 

Utilising a simple polynomial model to predict spectral fluxes is a distinctive
method from parallel efforts in radiative transfer of realistic (3D) stellar 
photospheres. Calculations of this nature are extremely expensive. Indeed, even
with simplifications of these models (e.g., interpolated 1D photospheres),
calculating synthetic spectra requires knowledge of species opacities, detailed 
atomic and molecular information for all contributing elements, solving for
the equation of state, and iteratively performing expensive radiative transfer
calculations. Given the
simplicity of a quadratic-in-labels spectral model with \tc{}, it is remarkable
that these complex calculations can be simplified so greatly, whilst maintaining
the typical measurement fidelity reported for high-resolution spectroscopic studies based on
\textit{curve-of-growth} analyses.

In a way, \tc{} is approximating spectral synthesis. Thus far the applications
have been limited to stellar spectra with spectral resolving powers of 20,000
or less. In this regime, the profiles of weak (linearly-responding) spectral lines
are well-represented by Gaussian profiles. In 

% The Cannon is approximating spectral synthesis. Done for R ~ 20k spectra
% Since all Gaussian and flux value is linearly related to EW, it is reasonable
% to expect the EW of an individual line can also be approximated with a
% model like TC.

Before extending this framework to include individual chemical abundances, it is
a useful exercise to understand \textit{why} a simple polynomial model can 
approximate spectral synthesis calculations with comparable fidelity. The reason
extends past model flexibility: one could imagine a similar coefficient vector
$\cv$ with a different functional form of the label vector $\lv$ (e.g., an
extreme case might be $\lv \equiv [1, {\rm [M/H]}^{T_{\rm eff}},
\log{g}^{T_{\rm eff}}]$) which would probably be incapable of describing
spectral fluxes with any fidelity.

% Describe the model of EW formation


% How this changes as a function of teff, logg, feh



Thus, we have a reasonable form of how EW, log(A) change with respect to
stellar parameters. At this point we have simply approximated the curve of growth
for a given line. The coefficients need to be solved for. These can be solved for
from existing data (fitting profiles) with measured abundances or by synthesising
spectra and relating the EW to log(X).

% Figure of relating EW to [X/H] with some simple model compared to analytic value.

Consider a scenario where the LSF is known and we want to model what the
spectrum looks like for three lines. For a given log(X) and stellar parameters,
we can solve for the EW. If the LSF is known, we can directly turn this into a
line profile under the assumption of gaussianity.

Multiple (separated) lines can be treated

Include h function.

If lines are blended, one could consider modelling many more atomic transitions.
However as the number grows, this situation becomes unnecessarily complex:
often blended lines exist and we do not care about their abundance, only their
inclusion (nuisance or dirty lines). Other features may be a complex combination
of thousands of molecular features.

The compromise is to use TC and `clean' lines that we care about in concert.

The spectral model becomes a multiplicative function of $g$ and $h$ with some
unknown noise.

\begin{equation}
    f_{n\lambda} = g(\lv\given\cv) \cdot h(\lv,\wv\given\psi_n) +  {\rm noise}.
\end{equation}

The vector $\wv$ can be calculated directly from spectral synthesis, or using data.
In practice, given some rest-frame normalised spectra and some knowledge of the
line spread function, $h(\lv,\wv\given{}X)$ can be calculated before training \tc.


% existing simple model. replaces expensive synthesis, involving model atmosphere
% generation, t-tau relations, opacities, equation of state, integrated line-of-sight
% and radiative transfer, etc.

% before extending this model to include individual abundances, it is worthwhile
% demonstrating (from a physics perspective) *why* the cannon works so well.

% APOGEE, R of 20,000, 
% LAMOST, higher Rs
% super high-resolution also works.



\section{APOGEE}

% All about APOGEE Survey and existing data.

% Apply a model using stellar parameters and abundances from X stars in clusters
% Typical S/N ratio and quality constraints employed
% Any abundance flags ignored?

% Train a model based on stellar parameters, include XYZ lines (Tabular data)


\section{Discussion}

% Comparison in EW model to log(X)
% Std deviation in this model.

% Training directly from data, or training from physical models?
% Inferences that can be made from models (when data are lacking)

% Requirements about knowing the LSF (e.g., a one-parameter LSF is just as good as TC)
% Blended lines

% Limitations: what about missing abundance data?

\section{Conclusions}

% 


\section*{Acknowledgments}
This research can be reproduced in full using the commands \texttt{pip install fireworks},
\texttt{fireworks-reproduce}. This article was produced using commit hash \texttt{hash}.

This research was fostered by discussions at the \textit{New Milky Way}
conference hosted by the Munich Institute for Astro- and Particle Physics, and
the \textit{Stellar Streams in the Local Universe} conference hosted at Ringberg
Castle by the Max-Planck-Institut F\"ur Astronomie. This research made use of 
Astropy, a community-developed core Python package for Astronomy \citep{astropy}.

\begin{thebibliography}{99}
\bibitem[Astropy Collaboration et al.(2013)]{astropy} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, \aap, 558, AA33

\bibitem[Ness et al.(2015)]{Ness2015} Ness, M., Hogg, D.~W., 
Rix, H.-W., Ho, A.~Y.~Q., \& Zasowski, G.\ 2015, \apj, 808, 16 
\end{thebibliography}


\label{lastpage}

\section{Appendix A: Reproducing this research}
This article has been compiled from the `git` repository hosted online at GitHub (commit hash \gitHash on `master` branch). The figures and tables included in this article can be reproduced in full using the following commands in a modern terminal:

git clone git@github.com:andycasey/fireworks.git fireworks
cd fireworks
python setup.py install
python reproduce.py
cd article
make

If the required tools are not available on the readers system (e.g., `git`), this research can also be produced using the public Docker image .

\end{document}

