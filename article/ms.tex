\documentclass[useAMS,usenatbib]{mn2e}

\usepackage{amsmath,amsfonts,amssymb,amsbsy}
\usepackage{gitinfo2}


% Convenient colloquiualisms.
\newcommand\article{\textit{Article}}
\newcommand\tc{\textit{The Cannon}}

% Convenient maths.
\newcommand\K{$\mathbf{K}$}
\newcommand\lv{\mathbf{\boldsymbol\ell_n}}
\newcommand\cv{{\boldsymbol\theta}_\lambda}
\newcommand\wv{\mathbf{w_n}}
\newcommand\ssq{s_\lambda^2}
\newcommand\given{|}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\newcommand\teff{$T_{\rm eff}$}
\newcommand\logg{$\log{g}$}
\newcommand\mh{${\mathrm [M/H]}$}

\title[Cannon Chemistry]{Detailed Chemical Abundances with \tc{}} 
% Some arrangement of the following authors:
\author[Casey et al.]{A.~R.~Casey$^1$, M.~Ness$^2$, G.~Gilmore$^1$,
    D.~W.~Hogg$^{2,3,4}$, H.~W.~Rix$^2$ \\ 
$^1$Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge
    CB3 0HA, UK\\
$^2$Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg,
    Germany\\
$^3$Center for Cosmology and Particle Physics, Department of Physics, New York
    University, 4 Washington Pl., room 424, New York, \\
    NY, 10003, USA\\
$^4$Center for Data Science, New York University, 726 Broadway, 7th Floor,
    New York, NY 10003, USA}
\begin{document}

\date{Accepted 2015 XX XX. Received 2015 October XX; in original form 2015 XX xx}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2015}

\maketitle

\label{firstpage}

\begin{abstract}
More than 15 million stellar spectra are expected to be acquired by ground-based surveys in
the coming decade. The detailed chemical abundance analysis of these data poses
a significant challenge. \tc{} provides a purely data-driven
approach to stellar parameter (label) determination. Cross-validation tests
demonstrate that the precision achievable is comparable to existing methods that
rely on model atmospheres and spectral synthesis, but \tc{} is $\sim$300,000 times
faster, making it perfectly applicable for surveys. Including individual abundances by extending \tc{} dimensionality can create enormous model difficulties, severely limiting the interpretability of the results. Here
we describe an alternative approach to including individual element abundances
in \tc{}. Using modest simplifications of the physics of spectral line formation,
we demonstrate precisely \textit{why} an approach like \tc{} works so effectively,
and provide a physically-motivated model that describes flux values, given the
stellar parameters. The inclusion of individual chemical abundances does not
impact on model flexibility, allowing for simultaneous training on detailed
chemical abundances (which are formed through well-understood interactions), as well as arbitrary labels where the impact on the spectra
is unknown or poorly understood (e.g., masses, ages). Crucially, we provide a direct link
between data-driven models and theory of model atmospheres and spectral line formation. We demonstrate that
detailed chemical abundances can be measured with a precision of 0.0X-0.0Y~dex
in Z APOGEE spectra on a modern CPU in just one minute. We present abundance
measurements of 15 elements in Z APOGEE spectra from DR12.
\end{abstract}

\begin{keywords}
\end{keywords}

\section{Introduction}
\label{sec:introduction}

Ground-based stellar spectroscopic surveys have enormous scientific potential. Key absorption features in stellar spectra permit inferences on the internal structure of stars and their detailed chemical composition. Given a large enough sample of stars, these inferences can directly guide or constrain models over a large range of physical scales: the formation of planets, stars and clusters, supernovae yields and nucleosynthetic pathways, as well as the large scale evolution of the Milky Way. For these reasons an increasing number of stellar spectroscopic surveys are being undertaken, with each survey generation increasing in sample size \citep{gaia_eso; galah; lamost; apogee; 4most}.

% Data have outgrown the methods.
In some ways, the spectroscopic data have outgrown the methods used to analyse them. High-resolution ($\mathcal{R} > 20,000$) stellar spectroscopy is colloquially considered to be a `dark art': analyses are carefully performed by an expert spectroscopist, who themselves were trained by an expert in exactly where the stellar continuum should be placed, and which atomic lines are considered 'clean' or 'dirty'. This work is equally impressive as it is subjective: the best spectroscopists host an encyclopaedic knowledge of atomic absorption lines with decades of experience, yet in blind-tests their decisions of where to place the continuum quickly becomes the most dominant source of systematic error.
% Lot of unextracted information in these spectra (e.g., mass, ages)

% Chemical abundance analysis takes time.
Classical spectroscopic analyses are time-intensive both in human and computing resources. The Gaia-ESO Survey has been particularly instrumental in minimising the human resources required for analysis: all analysis codes of FGK stars employed in Gaia-ESO are now completely automatised. The computing challenge remains: some analysis codes take up to one hour (single core) to determine stellar parameters (\teff, \logg, \mh) for one star. Given the data scale (e.g., $\sim$400 stars observed simultaneously with HERMES), this poses a modest computational problem.

% These analysis techniques are not optimal, either.
These analyses also have strict limitations in validity, and a range of options are available. The community standard is to perform chemical abundance analyses using 1D model photospheres under the assumption of local thermal equilibrium (LTE). There are a number of photospheric models available. Detailed information about the species (atomic and molecular) opacities is required, and partition functions are usually approximated from polynomials. Atomic data (e.g., oscillator strengths) must be well-known, and the treatment of Stark and XX broadening must be carefully included. Results are further sensitive to the manner in which the equation of state was solved. Simply put, there are many things to consider for a careful analysis.

% TC is in stark contrast to these methods
% It's fast, accurate and precise.

% It ignores physics.
\tc{} falls into the regime of supervised machine learning. The results obtained by \tc{} thus far are extremely impressive, particularly when it is considered that the model ignores everything learned about atomic physics and stellar spectra since the names' inconception 100 years ago. There is no knowledge of stars, no input on interactions between fields and particles, nothing considering spectral line formation, and no input of atomic data, opacities, partition functions or depth-dependent photospheric quantities. Only a polynomial model of stellar parameter labels is required in order to reasonably approximate expensive spectral syntheses. However because \tc{} is purely data-driven, there is a missing link to physics.

% Including chemical abundances into TC is non-trivial.


% stellar parameter and chemical abundance analysis take time
% equation of state, radiative transfer & spectral synthesis of multiple lines
% 1D model atmospheres, 3D models.
% introducing tc
% it's awesome: fast, accurate and precise (given the models published thus far)
% impressively it does all this whilst ignoring all advances made in stellar physics over the past 100 years.
% introducing chemical abundances into TC is not trivial
% high dimensionality

This \article{} is organised as follows. In Section \ref{sec:model} we briefly describe the existing foundations for \tc{} and demonstrate from a theoretical perspective as to why the model flexibility is sufficient to accurately represent spectral fluxes. We also outline how to include individual atomic lines within an existing \textit{Cannon} model. In Section \ref{sec:apogee} we apply a detailed chemical abundance \textit{Cannon} to the APOGEE Data Release (DR) 12 spectra. We discuss the suitability and limitations of the method in Section \ref{sec:discussion}. In Section \ref{sec:conclusion} we conclude with a summary of our findings.

\section{Model}
\label{sec:model}

\tc{} provides a purely data-driven approach to stellar label determination. The
spectral model describes the flux $f_{n\lambda}$ (for star $n$) at a pixel with rest-wavelength $\lambda$ to be a function
of a coefficient vector $\cv$, and a label vector $\lv$:

\begin{equation}
    f_{n\lambda} = g(\lv\given\cv) + {\rm noise}.
    \label{eq:cannon}
\end{equation}

This form is flexible: the label vector $\lv$ may be linear in just a few common
labels (e.g., \teff, \logg, \mh\footnote{\citet{Ness15a} use [Fe/H] in $\lv$, but for the 
sake of distinguishing between overall metallicity and individual abundances,
throughout this \article{} we opt for \mh to describe overall metallicity})
or a complex combination of all $K$ available labels, some of which may have an
 unknown or non-trivial relationship with spectral fluxes \citep[e.g., ages, 
 masses, see][]{Nissen15, Ness15b}. The underlying principle
is that the flux for any pixel from any star is predictable with a simple (usually
 a polynomial) model, given the labels. Indeed, \citet{Ness15a}
employ a quadratic-in-labels spectral model using stellar
parameters ($T_{\rm eff}$, $\log{g}$, [M/H])
and demonstrate through cross-validation that these labels can be 
measured with a precision of 95~K in $T_{\rm eff}$, 0.24 in $\log{g}$, 0.08 in 
[M/H], with negligible biases. 

The noise (Eq. \ref{eq:cannon}) can be taken as a quadrature combination of the pixel uncertainty variance $\sigma_{n\lambda}^2$ and the intrinsic variance of the spectral model at each wavelength $s_\lambda^2$. The flux $f_{n\lambda}$ can be written as a linear function of the label vector, leading to a single-pixel log-likelihood function

\begin{equation}
\ln{p}\left(f_{n\lambda}\given\cv^\intercal,\lv, s_{\lambda}^2\right) = -\frac{1}{2}\frac{\left[f_{n\lambda} - \cv^{\intercal}\cdot\lv\right]^2}{s_\lambda^2 + \sigma_{n\lambda}^2} -\frac{1}{2}\ln\left(s_{\lambda}^2 + \sigma_{n\lambda}^2\right).
\label{eq:pixel-log-likelihood}
\end{equation}

Given some sample of stars with known labels (the training set), the label vector $\lv$ and fluxes $f_{n\lambda}$ are known, allowing us to solve for the parameters $[\cv,s_\lambda^2]$ for each pixel with wavelength $\lambda$. Here we will optimise the negative log-likelihood (Equation \ref{eq:pixel-log-likelihood}). This is the training step of \tc{}, where the parameters $[\cv,s_\lambda^2]$ are optimised for a single pixel at $\lambda$ by considering all stars $N$:

\begin{equation}
\cv,s_\lambda \leftarrow^{\arg\,\max} {\cv, s_\lambda} \sum_{n=1}^N \ln{p}\left(f_{n\lambda}\given\cv^\intercal,\lv,s_\lambda^2\right)
\end{equation}

Once the coefficients $[\cv,\ssq]$ are known, \tc{} provides a \textit{generative model} for spectral fluxes. Through a cheap linear product of $\cv^\intercal$ and $\lv$, \tc{} is approximating spectral synthesis, an otherwise extremely expensive endeavour. We refer the reader to \citet{Ness15a} for a more detailed description of \tc{}. 

Before extending \tc{} to include individual chemical abundances, it is
a useful exercise to understand \textit{why} a simple spectral model can 
approximate synthesis calculations with comparable fidelity, and thus \textit{how} this framework can be extended to include individual abundances. At present the applications of \tc{} have been restricted to stellar spectra with resolving powers of $\mathcal{R} = 22,500$ \citep[APOGEE;][]{Ness15a} or less \citep[e.g., LAMOST;][]{Ho15}. In these regimes, the shapes of unsaturated spectral lines are well-represented by Gaussian profiles. This fact leads to several critical insights, which we describe below.

In stellar spectroscopy we are frequently presented with the idea of the equivalent width (EW; $\mathcal{W}$) of an absorption line. Consider a single (unblended) atomic line at rest wavelength $\lambda_r$, where the flux values at and surrounding $\lambda_r$ ($f_{n\lambda_r-{\delta}},f_{n\lambda_{r+\delta}}$) are well-approximated by a \textit{Cannon} spectral model. Because the absorption line is well-represented by a Gaussian profile at this spectral resolution, the flux at $\lambda_r$ is related to the EW of the line through

\begin{equation}
\mathcal{W} = \sigma{}\sqrt{2\pi}(1 - f_{n\lambda_r})
\label{eq:ew-flux}
\end{equation}

where $\sigma$ is the standard deviation of the profile, which is well-approximated by width of the line spread function (LSF) kernel. That is to say, for resolutions $\mathcal{R} \lesssim 20,000$, $\sigma$ is dominated by the spectrograph, not the absorption line or the labels (stellar parameters). If we presume that the LSF is well-known, or reasonably approximated by a fixed spectral resolution $\mathcal{R}$, then it is clear to see that \tc{} is implicitly modelling the EW of an atomic line as a function of stellar labels, and modelling spectral flux through a per-pixel scaling of $\sigma$. Indeed, the success of \tc{} suggests that the EW of an atomic line can also be well-approximated by a simple model relating the labels (i.e., stellar parameters and abundance of a single species). If such a model can be constructed and appropriately included within the existing \textit{Cannon} framework, it would permit the determination of detailed chemical abundances, whilst maintaining the flexibility in training on labels with poorly-known relationships on spectral fluxes (e.g., ages).

We note that Equation \ref{eq:ew-flux} does not imply that \tc{} is limited in applicability to low-resolution ($\mathcal{R} \lesssim 20,000$) spectra. Quite the opposite. \tc{} is sufficiently flexible that in extremely high-resolution spectra, where absorption lines are more closely represented by Lorentzian or Hertzting functions, the flux values $f_{n\lambda}$ in the wings of a strong line are presumably still well-modelled by \tc{}. Indeed, the line $\mathcal{W}$ would still be predictable with high-fidelity, implying that the coefficients $\cv$ for pixels in the wings just scale in a different way. Equation \ref{eq:ew-flux} allows us to directly translate $\mathcal{W}$ to $f_{n\lambda}$, thereby accounting for the flux contribution from a single atomic line. If the spectral resolving power were sufficiently high such that the absorption profiles appeared Lorentzian in nature, some prior information on the shape $\gamma$ would be required to translate $\mathcal{W}$ to $f_{n\lambda}$. Thus, while Equation \ref{eq:ew-flux} allows the translation from $\mathcal{W}$ to $f_{n\lambda}$ for $\mathcal{R} \lesssim 20,000$ spectra, given some knowledge of the LSF, no limiting interpretation about \tc{} should be drawn. Importantly, all future stellar spectroscopic surveys  currently expect $\mathcal{R} \lesssim 20,000$, in the perfect Gaussian-profile regime for this extension of \tc{}.

% Show an example? The example could have a figure showing some spectral fluxes for a clean line, which is modelled by the cannon (with an example model spectrum) and then show the same model being used to train on EW.
%An example of this is shown in Figure \ref{fig:cannon-ew-relation}, where a quadratic-in-labels \textit{Cannon} model is used to model spectral fluxes for X stars across the HR diagram. The data have been trained 

Knowing that a simple model can relate the a single line abundance [X/H] and integrated strength of an atomic line $\mathcal{W}$, given stellar parameter labels, it is intriguing to ask whether model atmosphere relations and the theory of spectral line formation can guide the level of model complexity required.

% How does EW of a line change w.r.t. stellar parameters
\textbf{Description of how EW for a single line will change w.r.t stellar parameters}

We now have a functional form of a model that relates $\mathcal{W}$ to $\log_\epsilon\left({\textrm X}\right)$ (or a scaled-solar [X/H] value), given the stellar parameters, which themselves are part of the set of $K$ labels. It is interesting to note that Equation \ref{eq:ew-logx-model} is not dissimilar to the quadratic-in-labels model used by \citet{Ness15a} for APOGEE data. With only the prior knowledge that \textit{a simple model of the labels can predict spectral fluxes}, we have derived the appropriate functional form for a single absorption line.

% How do we include this atomic line model.
% However this is complex because if we extended this to infinity, we would need an infinity of blended lines, etc.



%Utilising a simple polynomial model to predict spectral fluxes is a distinctive
%method from parallel efforts in radiative transfer of realistic (3D) stellar 
%photospheres. Calculations of this nature are extremely expensive. Indeed, even
%with simplifications of these models (e.g., interpolated 1D photospheres),
%calculating synthetic spectra requires knowledge of species opacities, detailed 
%atomic and molecular information for all contributing elements, solving for
%the equation of state, and iteratively performing expensive radiative transfer
%calculations. Given the
%simplicity of a quadratic-in-labels spectral model with \tc{}, it is remarkable
%that these complex calculations can be simplified so greatly, whilst maintaining
%the typical measurement fidelity reported for high-resolution spectroscopic studies based on
%\textit{curve-of-growth} analyses.


% Solving for coefficients and scatter term.



Thus, we have a reasonable form of how EW, log(A) change with respect to
stellar parameters. At this point we have simply approximated the curve of growth
for a given line. The coefficients need to be solved for. These can be solved for
from existing data (fitting profiles) with measured abundances or by synthesising
spectra and relating the EW to log(X).

% Figure of relating EW to [X/H] with some simple model compared to analytic value.

Consider a scenario where the LSF is known and we want to model what the
spectrum looks like for three lines. For a given log(X) and stellar parameters,
we can solve for the EW. If the LSF is known, we can directly turn this into a
line profile under the assumption of gaussianity.

Multiple (separated) lines can be treated

Include h function.

If lines are blended, one could consider modelling many more atomic transitions.
However as the number grows, this situation becomes unnecessarily complex:
often blended lines exist and we do not care about their abundance, only their
inclusion (nuisance or dirty lines). Other features may be a complex combination
of thousands of molecular features.

The compromise is to use TC and `clean' lines that we care about in concert.

The spectral model becomes a multiplicative function of $g$ and $h$ with some
unknown noise.

\begin{equation}
    f_{n\lambda} = g(\lv\given\cv) \cdot h(\lv,\wv\given\psi_n) +  {\rm noise}.
\end{equation}

The vector $\wv$ can be calculated directly from spectral synthesis, or using data.
In practice, given some rest-frame normalised spectra and some knowledge of the
line spread function, $h(\lv,\wv\given{}X)$ can be calculated before training \tc.


% existing simple model. replaces expensive synthesis, involving model atmosphere
% generation, t-tau relations, opacities, equation of state, integrated line-of-sight
% and radiative transfer, etc.

% before extending this model to include individual abundances, it is worthwhile
% demonstrating (from a physics perspective) *why* the cannon works so well.

% APOGEE, R of 20,000, 
% LAMOST, higher Rs
% super high-resolution also works.



\section{APOGEE}

% All about APOGEE Survey and existing data.

% Apply a model using stellar parameters and abundances from X stars in clusters
% Typical S/N ratio and quality constraints employed
% Any abundance flags ignored?

% Train a model based on stellar parameters, include XYZ lines (Tabular data)


\section{Discussion}

% Comparison in EW model to log(X)
% Std deviation in this model.

% Training directly from data, or training from physical models?
% Inferences that can be made from models (when data are lacking)

% Requirements about knowing the LSF (e.g., a one-parameter LSF is just as good as TC)
% Blended lines

% Limitations: what about missing abundance data?

\section{Conclusions}

% 


\section*{Acknowledgments}
This research was fostered by discussions at the \textit{New Milky Way}
conference hosted by the Munich Institute for Astro- and Particle Physics, and
the \textit{Stellar Streams in the Local Universe} conference hosted at Ringberg
Castle by the Max-Planck-Institut F\"ur Astronomie. This research made use of 
Astropy, a community-developed core Python package for Astronomy \citep{astropy}.
The authors are thankful for the following free online services which help make
science reproducibility easier: GitHub, Travis Continuous Integration, and Zenodo.

\begin{thebibliography}{99}
\bibitem[Astropy Collaboration et al.(2013)]{astropy} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, \aap, 558, AA33

\bibitem[Ho et al.(2015)]{Ho15} Ho, A., et al., in preparation
\bibitem[Ness et al.(2015a)]{Ness15a} Ness, M., Hogg, D.~W., Rix, H.-W., Ho, A.~Y.~Q., \& Zasowski, G.\ 2015, \apj, 808, 16 

\bibitem[Ness et al.(2015b)]{Ness15b} Ness, M., et al., in preparation

\bibitem[Nissen(2015)]{Nissen15} Nissen, P.~E.\ 2015, \aap, 579, A52 


\end{thebibliography}


\label{lastpage}

\section{Appendix A: Reproducing this research}
This \article{} has been compiled from the \texttt{git} repository (abbreviated commit hash \texttt{\gitAbbrevHash} on \texttt{\gitBranch} branch) hosted online by GitHub at \texttt{github.com/andycasey/fireworks}. The data required to reproduce this analysis is persistently hosted through Zenodo \citep{DATA}. The complete analysis, figures and tables included in this \article{} can be reproduced in full using the following commands in a modern Unix-based terminal:
\vspace{0.5em}

\noindent\texttt{[reproducibility commands]} 
\vspace{0.5em} \\
If the required tools are not available on the readers system (e.g., \texttt{git}), this research can also be produced using the public Docker image \texttt{[docker image reference]}.

\end{document}

